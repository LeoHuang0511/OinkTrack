# OinkTrack
OinkTrack is a large-scale multi-pig tracking dataset collected from real-world commercial farms.

It features over 5 hours of annotated videos across various lighting conditions (day, night, transitions), with dense groups of pigs under occlusions and appearance similarity challenges. The dataset includes 573,700 bounding boxes for 574 consistent identities, supporting research on long-term tracking and behavior analysis in precision livestock farming.

<div align="center"><img src="assets/demo.png" ></div>


## Appendix

<div align="center">

üìÑ <b>Supplementary Materials Available!</b> üìÑ  
Due to page limits of the paper, we provide extra materials in the  
<a href="https://drive.google.com/drive/folders/1v65Y7Na0eWmqpHiVWiX_PFSGxb7wBe6Q?usp=sharing" target="_blank"><b>Appendix (Google Drive)</b></a>

</div>


## Dataset
Download the dataset from [Google Drive](https://drive.google.com/drive/folders/1G9ygFt_G6f4SUguMmxCgKxRFgmVpmYSE?usp=drive_link).


Organize as follows:
~~~
{OinkTrack ROOT}
|-- OinkTrack
|   |-- C1D-1
|   |   |-- raw_videos
|   |   |   |-- 20241206_075959_080059
|   |   |       |-- 1733443200-1733443214.ts
|   |   |       |-- 1733443215-1733443229.ts
|   |   |       |-- ...
|   |   |-- annotation.json
|   |-- C1D-2
|   |-- C1D-3
|   |-- ...
|   |-- mask
        |-- C1_mask.png
        |-- C2_mask.png
~~~

Each folder under `OinkTrack` corresponds to a specific scene and lighting condition:

* `C1D-*`, `C1N-*`, `C1DN-*`, etc.:

  * `C1` and `C2` refer to different camera views .
  * `D` = Daytime, `N` = Nighttime, `DN` = Day-to-Night transition, `ND` = Night-to-Day transition.

## Build a MOT-format dataset

This repository ships a helper script, **`make_dataset.py`**, that converts the raw OinkTrack recordings ( `.ts` clips + `annotation.json`) into a standard [Dance Track](https://github.com/DanceTrack/DanceTrack) layout (`img1/`, `gt/gt.txt`, `seqinfo.ini`, 1 fps).

### 1¬†¬∑¬†Folder layout before running

~~~
project_root
|-- make_dataset.py          ‚Üê the script (keep here)
|-- OinkTrack               ‚Üê downloaded from Google Drive
~~~

### 2¬†¬∑¬†Quick start

```bash
# From project_root
python make_dataset.py
```
### 3 ¬∑ Output

After running make_dataset.py, the following folder structure will be generated:
~~~
project_root
|-- make_dataset.py
|-- OinkTrack 
|-- dataset # generated MOT-format dataset
~~~
The structure of dataset/ is as follows:
~~~

dataset
|-- train
|   |-- C1D-1
|   |   |-- img1
|   |   |   |-- 00000001.jpg
|   |   |   |-- 00000002.jpg
|   |   |   |-- ...
|   |   |-- gt
|   |   |   |-- gt.txt
|   |   |-- seqinfo.ini
|   |--...
|-- val
|   |-- ...
|-- test
|   |-- ...
|-- train_seqmap.txt
|-- val_seqmap.txt
|-- test_seqmap.txt


~~~
Each line in `gt.txt` contains:

```

<frame>, <id>, <bb_left>, <bb_top>, <bb_width>, <bb_height>, 1, 1, 1
```


### 4¬†¬∑¬†Customising the split

Open the header of **`make_dataset.py`** and edit the `SPLIT` dictionary:

```python
SPLIT: dict[str, list[str]] = {
    "train": ["C1D-1", "C1D-3", ...],
    "val":   [...],
    "test":  [...],
}
```

Save, rerun the script, and the output folders & `*_seqmap.txt` files will be regenerated accordingly.

## Evaluation

For the tracking-by-detection models, we provide the weights of detector pre-trained on OinkTrack. Additionally, we also provide the weights of transformer-based models pre-trained on OinkTrack, such as MOTR, MeMOTR, MOTIP. Download the weights from [here](https://drive.google.com/file/d/1dWI9Z3Apj86bp5JRljoEy17q7cAWDn93/view?usp=sharing).

We also provide model predictions on OinkTrack for direct evaluation and analysis. Download the predictions from [here](https://drive.google.com/file/d/1K908z9evHad4NPhLUZgW9bISK-x7QIaB/view?usp=sharing).

Each prediction is stored in the standard MOT-format `.txt` file with the following structure:

```
<frame>, <id>, <bb_left>, <bb_top>, <bb_width>, <bb_height>, <conf>, -1, -1, -1
```

For evaluating the predictions, we adopt **TrackEval**, a widely used tracking evaluation toolbox. All metrics (MOTA, HOTA, IDF1, etc.) reported in our paper are computed using TrackEval.
For more details on how to run evaluation, please refer to the [official TrackEval repository](https://github.com/JonathonLuiten/TrackEval).

## Demo

The following GIFs illustrate sample visualizations generated from the OinkTrack dataset annotations and demonstrate the performance of different tracking models.

### Groundtruth Visualizations

#### Daytime / Nighttime Sequences
<p align="center">
  <img src="assets/visualized_C2D.gif" height="250px">
  <img src="assets/visualized_C2N.gif" height="250px">
</p>

#### Day-to-Night / Night-to-Day Sequences
<p align="center">
  <img src="assets/visualized_C1DN.gif" height="250px">
  <img src="assets/visualized_C1ND.gif" height="250px">
</p>

### Model Predictions

#### ByteTrack
<div align="center">
  <img src="assets/predicted_ByteTrack.gif">
</div>

#### MeMOTR
<div align="center">
  <img src="assets/predicted_MeMOTR.gif">
</div>

#### MOTIP
<div align="center">
  <img src="assets/predicted_MOTIP.gif">
</div>




